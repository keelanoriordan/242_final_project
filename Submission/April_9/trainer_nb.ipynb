{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torchvision import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Load Data #############################\n",
    "\n",
    "def load_dataset(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "############################# Data Preprocessing #############################\n",
    "\n",
    "class NuCLSDataset(Dataset):\n",
    "    def __init__(self, X, y, mode='CNN', bkgd='black'):\n",
    "        super().__init__()\n",
    "        self.bkgd = bkgd\n",
    "        padded_X = X\n",
    "        if self.bkgd == 'black':\n",
    "            self.X = padded_X # just pad the input X's with black to the right dimension\n",
    "            # print(f'Shape of padded X: {self.X.shape}')\n",
    "        else: # change black pixels to average value fo pixels in the cropped image\n",
    "            for i in range(padded_X.shape[0]):\n",
    "                im = padded_X[i].transpose(1,2,0)\n",
    "                non_black_pixels = im[im.sum(axis=2) > 0]\n",
    "                average_color = np.mean(non_black_pixels, axis=0)\n",
    "                mask = np.all(im == [0, 0, 0], axis=-1)\n",
    "                im[mask] = average_color.astype(float)\n",
    "                padded_X[i] = im.transpose(2,0,1)\n",
    "            self.X = padded_X\n",
    "\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float)\n",
    "        self.y = torch.tensor(y, dtype=torch.float)\n",
    "        self.mode = mode\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'CNN':\n",
    "            return self.X[idx], self.y[idx] # CNN returns index, for logreg returns flttened image\n",
    "        else:\n",
    "            return self.X[idx].reshape(-1), self.y[idx]\n",
    "        \n",
    "############################# Model #############################\n",
    "class Cell_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # conv  \n",
    "        self.conv1 = nn.Conv2d(3,100, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(100,200, kernel_size=5, stride = 2)\n",
    "        self.conv3 = nn.Conv2d(200,300, kernel_size=5, stride = 2)\n",
    "\n",
    "        # pool \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=4, stride=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=4, stride=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=4, stride=1)\n",
    "\n",
    "        # activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "\n",
    "        # batch normalization\n",
    "        self.bn1 = nn.BatchNorm2d(100)\n",
    "        self.bn2 = nn.BatchNorm2d(200)\n",
    "        self.bn3 = nn.BatchNorm2d(300)\n",
    "\n",
    "        # flatten\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # fully connected layer\n",
    "        fcconst = 300 * 2 * 2\n",
    "        self.fc1 = nn.Linear(fcconst, fcconst)\n",
    "        self.fc2 = nn.Linear(fcconst, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x   \n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.densenet = models.densenet121(pretrained=False)\n",
    "\n",
    "        num_features = self.densenet.classifier.in_features\n",
    "        self.densenet.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):   \n",
    "        x = self.densenet(x)\n",
    "        return x\n",
    "\n",
    "############################# Training #############################\n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, opt_method, learning_rate, batch_size, epoch, l2):\n",
    "        self.device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        \n",
    "        if opt_method == \"adam\":\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=l2)\n",
    "        else:\n",
    "            raise NotImplementedError(\"This optimization is not supported\")\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def train(self, train_data, val_data, save_file):\n",
    "        train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        train_loss_list, train_acc_list = [], []\n",
    "        val_loss_list, val_acc_list = [], []\n",
    "        weights = self.model.state_dict()\n",
    "        lowest_val_loss = np.inf\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Write header to save file\n",
    "        with open(save_file, 'w') as f:\n",
    "            f.write(\"Epoch,Train Loss,Train Accuracy,Validation Loss,Validation Accuracy\\n\")\n",
    "\n",
    "        print(\"######################################################\")\n",
    "        print(\"###################### Training ######################\")\n",
    "        print(\"######################################################\")\n",
    "\n",
    "        for n in tqdm(range(self.epoch), leave=False, desc=\"Epochs\"):\n",
    "            self.model.train()\n",
    "            epoch_loss, epoch_acc = 0.0, 0.0\n",
    "            for X_batch, y_batch in tqdm(train_loader, leave=False, desc=\"Training Data\"):\n",
    "                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
    "                batch_importance = y_batch.shape[0] / len(train_data)\n",
    "                y_pred = self.model(X_batch)\n",
    "                batch_loss = loss_func(y_pred, y_batch)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss += batch_loss.detach().cpu().item() * batch_importance\n",
    "                batch_acc = torch.sum(torch.argmax(y_pred, axis=-1) == torch.argmax(y_batch, axis=-1)) / y_batch.shape[0]\n",
    "                epoch_acc += batch_acc.detach().cpu().item() * batch_importance\n",
    "            val_loss, val_acc = self.evaluate(val_data)\n",
    "            # write results to save file\n",
    "            with open(save_file, 'a') as f:\n",
    "                f.write(f\"{n+1},{epoch_loss},{epoch_acc},{val_loss},{val_acc}\\n\")\n",
    "            # print(f\"Epoch: {n+1}, Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_acc:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            \"train_loss_list\": train_loss_list,\n",
    "            \"train_acc_list\": train_acc_list,\n",
    "            \"val_loss_list\": val_loss_list,\n",
    "            \"val_acc_list\": val_acc_list,\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, data, print_acc=False):\n",
    "        self.model.eval()\n",
    "        loader = DataLoader(data, batch_size=self.batch_size, shuffle=True)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        acc, loss = 0.0, 0.0\n",
    "        for X_batch, y_batch in loader:\n",
    "            with torch.no_grad():\n",
    "                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
    "                batch_importance = y_batch.shape[0] / len(data)\n",
    "                y_pred = self.model(X_batch)\n",
    "                batch_loss = loss_func(y_pred, y_batch)\n",
    "                batch_acc = torch.sum(torch.argmax(y_pred, axis=-1) == torch.argmax(y_batch, axis=-1)) / y_batch.shape[0]\n",
    "                acc += batch_acc.detach().cpu().item() * batch_importance\n",
    "                loss += batch_loss.detach().cpu().item() * batch_importance\n",
    "        if print_acc:\n",
    "            print(f\"Accuracy: {acc:.3f}\")\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Set Parameters for training #########################\n",
    "\n",
    "#### Data ####\n",
    "data_dir = \"/Users/keelanoriordan/Desktop/GitHub_MacPro.nosync/242-Final-Project/DATASET/nuc\"\n",
    "nuc_file = os.path.join(data_dir, 'single_rater.pkl')\n",
    "\n",
    "#### Class ####\n",
    "class_map = {'raw_classification':0, \"main_classification\":1, 'super_classification':2}\n",
    "training_class = class_map['super_classification']\n",
    "\n",
    "#### Save Files ####\n",
    "results_dir = '/Users/keelanoriordan/Desktop/GitHub_MacPro.nosync/242-Final-Project/DATASET/results'\n",
    "results_path = os.path.join(results_dir, 'results.txt')\n",
    "model_params_path = os.path.join(results_dir, 'summary.txt')\n",
    "\n",
    "#### Model and Parameters ####\n",
    "model = DenseNet(num_classes=3)\n",
    "# model = Cell_CNN(num_classes=3)\n",
    "# param = [model, opt_method, learning_rate, batch_size, epoch, l2]\n",
    "param = [model, \"adam\", 1e-4, 128, 100, 1e-5]\n",
    "\n",
    "\n",
    "\n",
    "# load data\n",
    "data = load_dataset(nuc_file)\n",
    "X = np.array([data[i][0] for i in range(len(data))])\n",
    "y = np.array([data[i][1][training_class] for i in range(len(data))])\n",
    "encoder = OneHotEncoder()\n",
    "y_enc = encoder.fit_transform(y.reshape(-1,1)).toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.2, shuffle=True)\n",
    "\n",
    "train_data = NuCLSDataset(X=X_train, y=y_train, mode='CNN', bkgd='avg')\n",
    "test_data = NuCLSDataset(X=X_test, y=y_test, mode='CNN', bkgd='avg')\n",
    "\n",
    "# train model\n",
    "# Save model information\n",
    "# Save the original standard output\n",
    "original_stdout = sys.stdout \n",
    "with open(model_params_path, 'w') as f:\n",
    "    f.write(f\"Training Dataset: {nuc_file}\\n\")\n",
    "    f.write(f\"Model Hyperparameters: Optimizer = {param[1]}, Learning Rate4 = {param[2]}, Batch Size = {param[3]}, epoch = {param[4]}, l2 norm = {param[5]}\\n\")\n",
    "    sys.stdout = f # Change the standard output to the file we created.\n",
    "    ds = summary(model, (3, 80, 80))\n",
    "    sys.stdout = original_stdout # Reset the standard output to its original value\n",
    "\n",
    "trainer = Trainer(param[0], param[1], param[2], param[3], param[4], param[5])\n",
    "trainer.train(train_data, test_data, results_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
